ğŸ Reinforcement-Learning Snake

A modular, RL-ready Snake environment with DQN training and agent visualization

This project implements a full reinforcement-learning environment for the classic Snake game â€” complete with a Gym-like API, modular game logic, multiple policies (random, greedy, Îµ-greedy), and a full Deep Q-Network (DQN) training pipeline.

The goal of the project is learning by building:
from raw game mechanics â†’ to environment design â†’ to training deep RL agents â†’ to live visualization of the trained agent playing in a pygame window.

----------------------------------------------------------------------------------------

rl-snake/
â”‚
â”œâ”€â”€ assets/                 # images, icons (optional)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ runs/               # training logs + saved models
â”‚
â”œâ”€â”€ models/                 # optional model export directory
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ snake/              # Pure game logic (no RL)
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ game.py
â”‚   â”‚   â””â”€â”€ main.py         # human playable version
â”‚   â”‚
â”‚   â””â”€â”€ rl/
â”‚       â”œâ”€â”€ env.py          # Gym-like Snake RL environment
â”‚       â”œâ”€â”€ train.py        # training & evaluation entrypoint
â”‚       â”œâ”€â”€ agents/
â”‚       â”‚   â””â”€â”€ dqn.py      # Deep Q-Network agent
â”‚       â””â”€â”€ policies/
â”‚           â”œâ”€â”€ random.py
â”‚           â”œâ”€â”€ greedy.py
â”‚           â””â”€â”€ eps_greedy.py
â”‚
â””â”€â”€ README.md

----------------------------------------------------------------------------------------

ğŸ¯ Observation Space

Each state is a 9-dimensional vector:

| Index | Feature        | Meaning                         |
| ----- | -------------- | ------------------------------- |
| 0     | `hx_n`         | head x (normalized 0â€“1)         |
| 1     | `hy_n`         | head y (normalized 0â€“1)         |
| 2     | `fx_n`         | food x (normalized 0â€“1)         |
| 3     | `fy_n`         | food y (normalized 0â€“1)         |
| 4     | `dx`           | direction x (âˆ’1, 0, 1)          |
| 5     | `dy`           | direction y (âˆ’1, 0, 1)          |
| 6     | `danger_ahead` | 1 if next move forward is fatal |
| 7     | `danger_left`  | 1 if left turn is fatal         |
| 8     | `danger_right` | 1 if right turn is fatal        |

----------------------------------------------------------------------------------------

ğŸ† Reward Function
| Event             | Reward                       |
| Eat food	        | +1.0                         |
| Die               | -1.0                         |
|Step penalty       | -0.001                       |
Move closer to food	| +0.01 * (d_before - d_after) |

Reward shaping encourages exploration and reduces wandering, while still letting the agent learn strategic behavior.

----------------------------------------------------------------------------------------

ğŸš€ Training

Train a DQN agent:

python -m src.rl.train --policy dqn --episodes 3000

This will produce:
-- CSV logs under data/runs/rl_dqn.csv
-- A saved model under data/runs/rl_dqn_dqn.pt

View episode logs live in the console:
ep, steps , return , score
1 , 210   , -0.520 , 2
2 , 195   , -0.489 , 1

----------------------------------------------------------------------------------------

ğŸ“Š Future Improvements

Environment Improvements:
-- Add a â€œlocal gridâ€ observation (5Ã—5 or 7Ã—7 vision
-- RL Improvements
-- Double DQN
-- Dueling DQN
-- Policy-gradient agents (PPO, A2C)
-- TensorBoard logging
-- Live performance dashboard

ğŸ“ License
MIT â€” free to use, modify, and distribute.

ğŸ¤ Contributing -- Contributions are welcome â€” especially around:

-- improving the agent architecture
-- extending the observation space
-- adding alternative RL algorithms
